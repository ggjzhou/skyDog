#连接数据库
spring:
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://127.0.0.1:3306/db_skydog?useUnicode=true&characterEncoding=utf-8&allowMultiQueries=true
    username: root
    password: root
    type: com.zaxxer.hikari.HikariDataSource # 数据池
    hikari:
      # 池中维护的最小连接数
      minimum-idle: 10
      # 池中最大连接数，包括闲置和使用的连接数
      maxmum-pool-size: 20
      # 池中连接最长生命周期，默认为30分钟
      max-lifetime: 1800000
      # 允许最长空闲时间
      idle-timeout: 30000
      # 数据库连接超时时间，默认为30秒
      connection-timeout: 30000
  #kafka
  kafka:
    #集群地址
    bootstrap-servers: master:9092,slave01:9092,slave02:9092
    #生产者配置
    producer:
      #系列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      #采用的ack机制
      acks: 1
      #批量提交的数据大小 16kb
      batch-size: 16384
      #生产者暂存数据的缓冲区大小
      buffer-memory: 33554432
    #消费者配置
    consumer:
      #是否自动提交偏移量
      enable-auto-commit: true
      #消费消息后间隔多长时间提交偏移量
      auto-commit-interval: 100
      #默认的消费者组，代码中可以热键修改
      group-id: test824
      # earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
      # latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
      # none:topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
      auto-offset-reset: latest
      #系列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  mvc:
    format:
      date: yyyy-MM-dd
    pathmatch:
      matching-strategy: ant_path_matcher
    throw-exception-if-no-handler-found: true
  jackson:
    date-format: yyyy-MM-dd
    time-zone: GMT+8

#整合mybatis
mybatis:
  #加载实体类
  type-aliases-package: com.example.skydog.module.entity
  #映射文件
  mapper-locations: mapper/*Mapper.xml
  #输出日志
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl

server:
  port: 8088




